I1118 21:21:59.981039 27838 caffe.cpp:218] Using GPUs 0
I1118 21:22:00.007007 27838 caffe.cpp:223] GPU 0: GeForce GTX 1080 Ti
I1118 21:22:00.368607 27838 solver.cpp:44] Initializing solver from parameters: 
base_lr: 0.001
display: 20
max_iter: 10000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 3000
snapshot: 100
snapshot_prefix: "/home/hzzone/1tb/id-model/classification/3d/3d_alexnet/3d_classification_alexnet_train"
solver_mode: GPU
device_id: 0
net: "./train_val.prototxt"
train_state {
  level: 0
  stage: ""
}
I1118 21:22:00.368846 27838 solver.cpp:87] Creating training net from net file: ./train_val.prototxt
I1118 21:22:00.369379 27838 net.cpp:51] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "../train.txt"
    batch_size: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution3D"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution3d_param {
    num_output: 96
    pad: 1
    kernel_size: 3
    kernel_depth: 3
    stride: 1
    temporal_stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    temporal_pad: 1
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling3D"
  bottom: "conv1"
  top: "pool1"
  pooling3d_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    kernel_depth: 2
    temporal_stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution3D"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution3d_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    kernel_depth: 3
    stride: 1
    temporal_stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    temporal_pad: 1
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling3D"
  bottom: "conv2"
  top: "pool2"
  pooling3d_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    kernel_depth: 2
    temporal_stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution3D"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution3d_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    kernel_depth: 3
    stride: 1
    temporal_stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    temporal_pad: 1
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution3D"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution3d_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    kernel_depth: 3
    stride: 1
    temporal_stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    temporal_pad: 1
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution3D"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution3d_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    kernel_depth: 3
    stride: 1
    temporal_stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    temporal_pad: 1
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling3D"
  bottom: "conv5"
  top: "pool5"
  pooling3d_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    kernel_depth: 2
    temporal_stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1024
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1024
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 70
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1118 21:22:00.369500 27838 layer_factory.hpp:77] Creating layer data
I1118 21:22:00.369514 27838 net.cpp:84] Creating Layer data
I1118 21:22:00.369523 27838 net.cpp:380] data -> data
I1118 21:22:00.369546 27838 net.cpp:380] data -> label
I1118 21:22:00.369560 27838 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: ../train.txt
I1118 21:22:00.369668 27838 hdf5_data_layer.cpp:93] Number of HDF5 files: 146
I1118 21:22:00.370824 27838 hdf5.cpp:32] Datatype class: H5T_FLOAT
I1118 21:22:00.377997 27838 net.cpp:122] Setting up data
I1118 21:22:00.378041 27838 net.cpp:129] Top shape: 2 1 40 120 120 (1152000)
I1118 21:22:00.378049 27838 net.cpp:129] Top shape: 2 (2)
I1118 21:22:00.378053 27838 net.cpp:137] Memory required for data: 4608008
I1118 21:22:00.378067 27838 layer_factory.hpp:77] Creating layer conv1
I1118 21:22:00.378089 27838 net.cpp:84] Creating Layer conv1
I1118 21:22:00.378108 27838 net.cpp:406] conv1 <- data
I1118 21:22:00.378125 27838 net.cpp:380] conv1 -> conv1
I1118 21:22:00.382464 27838 net.cpp:122] Setting up conv1
I1118 21:22:00.382514 27838 net.cpp:129] Top shape: 2 96 40 120 120 (110592000)
I1118 21:22:00.382520 27838 net.cpp:137] Memory required for data: 446976008
I1118 21:22:00.382566 27838 layer_factory.hpp:77] Creating layer relu1
I1118 21:22:00.382609 27838 net.cpp:84] Creating Layer relu1
I1118 21:22:00.382632 27838 net.cpp:406] relu1 <- conv1
I1118 21:22:00.382656 27838 net.cpp:367] relu1 -> conv1 (in-place)
I1118 21:22:00.382683 27838 net.cpp:122] Setting up relu1
I1118 21:22:00.382702 27838 net.cpp:129] Top shape: 2 96 40 120 120 (110592000)
I1118 21:22:00.382719 27838 net.cpp:137] Memory required for data: 889344008
I1118 21:22:00.382736 27838 layer_factory.hpp:77] Creating layer pool1
I1118 21:22:00.382758 27838 net.cpp:84] Creating Layer pool1
I1118 21:22:00.382776 27838 net.cpp:406] pool1 <- conv1
I1118 21:22:00.382794 27838 net.cpp:380] pool1 -> pool1
I1118 21:22:00.382884 27838 net.cpp:122] Setting up pool1
I1118 21:22:00.382894 27838 net.cpp:129] Top shape: 2 96 20 60 60 (13824000)
I1118 21:22:00.382899 27838 net.cpp:137] Memory required for data: 944640008
I1118 21:22:00.382903 27838 layer_factory.hpp:77] Creating layer conv2
I1118 21:22:00.382915 27838 net.cpp:84] Creating Layer conv2
I1118 21:22:00.382920 27838 net.cpp:406] conv2 <- pool1
I1118 21:22:00.382925 27838 net.cpp:380] conv2 -> conv2
I1118 21:22:00.394145 27838 net.cpp:122] Setting up conv2
I1118 21:22:00.394191 27838 net.cpp:129] Top shape: 2 256 20 60 60 (36864000)
I1118 21:22:00.394196 27838 net.cpp:137] Memory required for data: 1092096008
I1118 21:22:00.394253 27838 layer_factory.hpp:77] Creating layer relu2
I1118 21:22:00.394271 27838 net.cpp:84] Creating Layer relu2
I1118 21:22:00.394279 27838 net.cpp:406] relu2 <- conv2
I1118 21:22:00.394289 27838 net.cpp:367] relu2 -> conv2 (in-place)
I1118 21:22:00.394306 27838 net.cpp:122] Setting up relu2
I1118 21:22:00.394315 27838 net.cpp:129] Top shape: 2 256 20 60 60 (36864000)
I1118 21:22:00.394320 27838 net.cpp:137] Memory required for data: 1239552008
I1118 21:22:00.394323 27838 layer_factory.hpp:77] Creating layer pool2
I1118 21:22:00.394332 27838 net.cpp:84] Creating Layer pool2
I1118 21:22:00.394337 27838 net.cpp:406] pool2 <- conv2
I1118 21:22:00.394345 27838 net.cpp:380] pool2 -> pool2
I1118 21:22:00.394382 27838 net.cpp:122] Setting up pool2
I1118 21:22:00.394392 27838 net.cpp:129] Top shape: 2 256 10 30 30 (4608000)
I1118 21:22:00.394397 27838 net.cpp:137] Memory required for data: 1257984008
I1118 21:22:00.394400 27838 layer_factory.hpp:77] Creating layer conv3
I1118 21:22:00.394412 27838 net.cpp:84] Creating Layer conv3
I1118 21:22:00.394415 27838 net.cpp:406] conv3 <- pool2
I1118 21:22:00.394423 27838 net.cpp:380] conv3 -> conv3
I1118 21:22:00.433550 27838 net.cpp:122] Setting up conv3
I1118 21:22:00.433598 27838 net.cpp:129] Top shape: 2 384 10 30 30 (6912000)
I1118 21:22:00.433604 27838 net.cpp:137] Memory required for data: 1285632008
I1118 21:22:00.433630 27838 layer_factory.hpp:77] Creating layer relu3
I1118 21:22:00.433648 27838 net.cpp:84] Creating Layer relu3
I1118 21:22:00.433656 27838 net.cpp:406] relu3 <- conv3
I1118 21:22:00.433668 27838 net.cpp:367] relu3 -> conv3 (in-place)
I1118 21:22:00.433681 27838 net.cpp:122] Setting up relu3
I1118 21:22:00.433689 27838 net.cpp:129] Top shape: 2 384 10 30 30 (6912000)
I1118 21:22:00.433694 27838 net.cpp:137] Memory required for data: 1313280008
I1118 21:22:00.433698 27838 layer_factory.hpp:77] Creating layer conv4
I1118 21:22:00.433710 27838 net.cpp:84] Creating Layer conv4
I1118 21:22:00.433715 27838 net.cpp:406] conv4 <- conv3
I1118 21:22:00.433723 27838 net.cpp:380] conv4 -> conv4
I1118 21:22:00.487207 27838 net.cpp:122] Setting up conv4
I1118 21:22:00.487246 27838 net.cpp:129] Top shape: 2 384 10 30 30 (6912000)
I1118 21:22:00.487252 27838 net.cpp:137] Memory required for data: 1340928008
I1118 21:22:00.487270 27838 layer_factory.hpp:77] Creating layer relu4
I1118 21:22:00.487283 27838 net.cpp:84] Creating Layer relu4
I1118 21:22:00.487290 27838 net.cpp:406] relu4 <- conv4
I1118 21:22:00.487300 27838 net.cpp:367] relu4 -> conv4 (in-place)
I1118 21:22:00.487311 27838 net.cpp:122] Setting up relu4
I1118 21:22:00.487319 27838 net.cpp:129] Top shape: 2 384 10 30 30 (6912000)
I1118 21:22:00.487324 27838 net.cpp:137] Memory required for data: 1368576008
I1118 21:22:00.487329 27838 layer_factory.hpp:77] Creating layer conv5
I1118 21:22:00.487339 27838 net.cpp:84] Creating Layer conv5
I1118 21:22:00.487344 27838 net.cpp:406] conv5 <- conv4
I1118 21:22:00.487352 27838 net.cpp:380] conv5 -> conv5
I1118 21:22:00.540588 27838 net.cpp:122] Setting up conv5
I1118 21:22:00.540633 27838 net.cpp:129] Top shape: 2 384 10 30 30 (6912000)
I1118 21:22:00.540642 27838 net.cpp:137] Memory required for data: 1396224008
I1118 21:22:00.540664 27838 layer_factory.hpp:77] Creating layer relu5
I1118 21:22:00.540678 27838 net.cpp:84] Creating Layer relu5
I1118 21:22:00.540685 27838 net.cpp:406] relu5 <- conv5
I1118 21:22:00.540695 27838 net.cpp:367] relu5 -> conv5 (in-place)
I1118 21:22:00.540706 27838 net.cpp:122] Setting up relu5
I1118 21:22:00.540714 27838 net.cpp:129] Top shape: 2 384 10 30 30 (6912000)
I1118 21:22:00.540719 27838 net.cpp:137] Memory required for data: 1423872008
I1118 21:22:00.540724 27838 layer_factory.hpp:77] Creating layer pool5
I1118 21:22:00.540733 27838 net.cpp:84] Creating Layer pool5
I1118 21:22:00.540740 27838 net.cpp:406] pool5 <- conv5
I1118 21:22:00.540746 27838 net.cpp:380] pool5 -> pool5
I1118 21:22:00.540772 27838 net.cpp:122] Setting up pool5
I1118 21:22:00.540809 27838 net.cpp:129] Top shape: 2 384 5 15 15 (864000)
I1118 21:22:00.540839 27838 net.cpp:137] Memory required for data: 1427328008
I1118 21:22:00.540844 27838 layer_factory.hpp:77] Creating layer fc6
I1118 21:22:00.540854 27838 net.cpp:84] Creating Layer fc6
I1118 21:22:00.540859 27838 net.cpp:406] fc6 <- pool5
I1118 21:22:00.540868 27838 net.cpp:380] fc6 -> fc6
I1118 21:22:06.345710 27838 net.cpp:122] Setting up fc6
I1118 21:22:06.345746 27838 net.cpp:129] Top shape: 2 1024 (2048)
I1118 21:22:06.345752 27838 net.cpp:137] Memory required for data: 1427336200
I1118 21:22:06.345767 27838 layer_factory.hpp:77] Creating layer relu6
I1118 21:22:06.345782 27838 net.cpp:84] Creating Layer relu6
I1118 21:22:06.345788 27838 net.cpp:406] relu6 <- fc6
I1118 21:22:06.345798 27838 net.cpp:367] relu6 -> fc6 (in-place)
I1118 21:22:06.345824 27838 net.cpp:122] Setting up relu6
I1118 21:22:06.345832 27838 net.cpp:129] Top shape: 2 1024 (2048)
I1118 21:22:06.345837 27838 net.cpp:137] Memory required for data: 1427344392
I1118 21:22:06.345841 27838 layer_factory.hpp:77] Creating layer drop6
I1118 21:22:06.345855 27838 net.cpp:84] Creating Layer drop6
I1118 21:22:06.345862 27838 net.cpp:406] drop6 <- fc6
I1118 21:22:06.345870 27838 net.cpp:367] drop6 -> fc6 (in-place)
I1118 21:22:06.345896 27838 net.cpp:122] Setting up drop6
I1118 21:22:06.345903 27838 net.cpp:129] Top shape: 2 1024 (2048)
I1118 21:22:06.345908 27838 net.cpp:137] Memory required for data: 1427352584
I1118 21:22:06.345912 27838 layer_factory.hpp:77] Creating layer fc7
I1118 21:22:06.345950 27838 net.cpp:84] Creating Layer fc7
I1118 21:22:06.345966 27838 net.cpp:406] fc7 <- fc6
I1118 21:22:06.345978 27838 net.cpp:380] fc7 -> fc7
I1118 21:22:06.361135 27838 net.cpp:122] Setting up fc7
I1118 21:22:06.361222 27838 net.cpp:129] Top shape: 2 1024 (2048)
I1118 21:22:06.361241 27838 net.cpp:137] Memory required for data: 1427360776
I1118 21:22:06.361273 27838 layer_factory.hpp:77] Creating layer relu7
I1118 21:22:06.361302 27838 net.cpp:84] Creating Layer relu7
I1118 21:22:06.361323 27838 net.cpp:406] relu7 <- fc7
I1118 21:22:06.361349 27838 net.cpp:367] relu7 -> fc7 (in-place)
I1118 21:22:06.361377 27838 net.cpp:122] Setting up relu7
I1118 21:22:06.361397 27838 net.cpp:129] Top shape: 2 1024 (2048)
I1118 21:22:06.361414 27838 net.cpp:137] Memory required for data: 1427368968
I1118 21:22:06.361433 27838 layer_factory.hpp:77] Creating layer drop7
I1118 21:22:06.361456 27838 net.cpp:84] Creating Layer drop7
I1118 21:22:06.361474 27838 net.cpp:406] drop7 <- fc7
I1118 21:22:06.361496 27838 net.cpp:367] drop7 -> fc7 (in-place)
I1118 21:22:06.361537 27838 net.cpp:122] Setting up drop7
I1118 21:22:06.361557 27838 net.cpp:129] Top shape: 2 1024 (2048)
I1118 21:22:06.361574 27838 net.cpp:137] Memory required for data: 1427377160
I1118 21:22:06.361591 27838 layer_factory.hpp:77] Creating layer fc8
I1118 21:22:06.361614 27838 net.cpp:84] Creating Layer fc8
I1118 21:22:06.361631 27838 net.cpp:406] fc8 <- fc7
I1118 21:22:06.361654 27838 net.cpp:380] fc8 -> fc8
I1118 21:22:06.362781 27838 net.cpp:122] Setting up fc8
I1118 21:22:06.362857 27838 net.cpp:129] Top shape: 2 70 (140)
I1118 21:22:06.362877 27838 net.cpp:137] Memory required for data: 1427377720
I1118 21:22:06.362911 27838 layer_factory.hpp:77] Creating layer loss
I1118 21:22:06.362938 27838 net.cpp:84] Creating Layer loss
I1118 21:22:06.362960 27838 net.cpp:406] loss <- fc8
I1118 21:22:06.362983 27838 net.cpp:406] loss <- label
I1118 21:22:06.363009 27838 net.cpp:380] loss -> loss
I1118 21:22:06.363054 27838 layer_factory.hpp:77] Creating layer loss
I1118 21:22:06.363183 27838 net.cpp:122] Setting up loss
I1118 21:22:06.363203 27838 net.cpp:129] Top shape: (1)
I1118 21:22:06.363220 27838 net.cpp:132]     with loss weight 1
I1118 21:22:06.363273 27838 net.cpp:137] Memory required for data: 1427377724
I1118 21:22:06.363291 27838 net.cpp:198] loss needs backward computation.
I1118 21:22:06.363308 27838 net.cpp:198] fc8 needs backward computation.
I1118 21:22:06.363325 27838 net.cpp:198] drop7 needs backward computation.
I1118 21:22:06.363359 27838 net.cpp:198] relu7 needs backward computation.
I1118 21:22:06.363391 27838 net.cpp:198] fc7 needs backward computation.
I1118 21:22:06.363409 27838 net.cpp:198] drop6 needs backward computation.
I1118 21:22:06.363425 27838 net.cpp:198] relu6 needs backward computation.
I1118 21:22:06.363440 27838 net.cpp:198] fc6 needs backward computation.
I1118 21:22:06.363457 27838 net.cpp:198] pool5 needs backward computation.
I1118 21:22:06.363476 27838 net.cpp:198] relu5 needs backward computation.
I1118 21:22:06.363492 27838 net.cpp:198] conv5 needs backward computation.
I1118 21:22:06.363510 27838 net.cpp:198] relu4 needs backward computation.
I1118 21:22:06.363528 27838 net.cpp:198] conv4 needs backward computation.
I1118 21:22:06.363546 27838 net.cpp:198] relu3 needs backward computation.
I1118 21:22:06.363562 27838 net.cpp:198] conv3 needs backward computation.
I1118 21:22:06.363579 27838 net.cpp:198] pool2 needs backward computation.
I1118 21:22:06.363596 27838 net.cpp:198] relu2 needs backward computation.
I1118 21:22:06.363613 27838 net.cpp:198] conv2 needs backward computation.
I1118 21:22:06.363634 27838 net.cpp:198] pool1 needs backward computation.
I1118 21:22:06.363651 27838 net.cpp:198] relu1 needs backward computation.
I1118 21:22:06.363665 27838 net.cpp:198] conv1 needs backward computation.
I1118 21:22:06.363682 27838 net.cpp:200] data does not need backward computation.
I1118 21:22:06.363696 27838 net.cpp:242] This network produces output loss
I1118 21:22:06.363726 27838 net.cpp:255] Network initialization done.
I1118 21:22:06.363828 27838 solver.cpp:56] Solver scaffolding done.
I1118 21:22:06.364504 27838 caffe.cpp:248] Starting Optimization
I1118 21:22:06.364555 27838 solver.cpp:273] Solving AlexNet
I1118 21:22:06.364573 27838 solver.cpp:274] Learning Rate Policy: step
I1118 21:22:07.121031 27838 solver.cpp:219] Iteration 0 (6.05331e+20 iter/s, 0.756307s/20 iters), loss = 4.01473
I1118 21:22:07.121121 27838 solver.cpp:238]     Train net output #0: loss = 4.01473 (* 1 = 4.01473 loss)
I1118 21:22:07.121160 27838 sgd_solver.cpp:105] Iteration 0, lr = 0.001
I1118 21:22:19.532420 27838 solver.cpp:219] Iteration 20 (1.61152 iter/s, 12.4107s/20 iters), loss = 4.40032
I1118 21:22:19.532496 27838 solver.cpp:238]     Train net output #0: loss = 4.40032 (* 1 = 4.40032 loss)
I1118 21:22:19.532508 27838 sgd_solver.cpp:105] Iteration 20, lr = 0.001
I1118 21:22:31.464630 27838 solver.cpp:219] Iteration 40 (1.67623 iter/s, 11.9316s/20 iters), loss = 4.13777
I1118 21:22:31.464804 27838 solver.cpp:238]     Train net output #0: loss = 4.13777 (* 1 = 4.13777 loss)
I1118 21:22:31.464817 27838 sgd_solver.cpp:105] Iteration 40, lr = 0.001
I1118 21:22:43.435700 27838 solver.cpp:219] Iteration 60 (1.6708 iter/s, 11.9703s/20 iters), loss = 4.32238
I1118 21:22:43.435763 27838 solver.cpp:238]     Train net output #0: loss = 4.32238 (* 1 = 4.32238 loss)
I1118 21:22:43.435775 27838 sgd_solver.cpp:105] Iteration 60, lr = 0.001
I1118 21:22:55.428668 27838 solver.cpp:219] Iteration 80 (1.66773 iter/s, 11.9923s/20 iters), loss = 4.21942
I1118 21:22:55.428742 27838 solver.cpp:238]     Train net output #0: loss = 4.21942 (* 1 = 4.21942 loss)
I1118 21:22:55.428755 27838 sgd_solver.cpp:105] Iteration 80, lr = 0.001
I1118 21:23:06.447161 27838 solver.cpp:448] Snapshotting to binary proto file /home/hzzone/1tb/id-model/classification/3d/3d_alexnet/3d_classification_alexnet_train_iter_100.caffemodel
I1118 21:23:18.201769 27838 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/hzzone/1tb/id-model/classification/3d/3d_alexnet/3d_classification_alexnet_train_iter_100.solverstate
I1118 21:23:31.081809 27838 solver.cpp:219] Iteration 100 (0.560987 iter/s, 35.6515s/20 iters), loss = 4.43424
I1118 21:23:31.081876 27838 solver.cpp:238]     Train net output #0: loss = 4.43424 (* 1 = 4.43424 loss)
I1118 21:23:31.081887 27838 sgd_solver.cpp:105] Iteration 100, lr = 0.001
I1118 21:23:43.041009 27838 solver.cpp:219] Iteration 120 (1.67244 iter/s, 11.9586s/20 iters), loss = 4.36916
I1118 21:23:43.041348 27838 solver.cpp:238]     Train net output #0: loss = 4.36916 (* 1 = 4.36916 loss)
I1118 21:23:43.041360 27838 sgd_solver.cpp:105] Iteration 120, lr = 0.001
I1118 21:23:54.971782 27838 solver.cpp:219] Iteration 140 (1.67646 iter/s, 11.9299s/20 iters), loss = 4.4355
I1118 21:23:54.971853 27838 solver.cpp:238]     Train net output #0: loss = 4.4355 (* 1 = 4.4355 loss)
I1118 21:23:54.971865 27838 sgd_solver.cpp:105] Iteration 140, lr = 0.001
I1118 21:24:06.921558 27838 solver.cpp:219] Iteration 160 (1.67376 iter/s, 11.9492s/20 iters), loss = 4.03905
I1118 21:24:06.921627 27838 solver.cpp:238]     Train net output #0: loss = 4.03905 (* 1 = 4.03905 loss)
I1118 21:24:06.921638 27838 sgd_solver.cpp:105] Iteration 160, lr = 0.001
I1118 21:24:18.876068 27838 solver.cpp:219] Iteration 180 (1.67309 iter/s, 11.9539s/20 iters), loss = 4.50154
I1118 21:24:18.876200 27838 solver.cpp:238]     Train net output #0: loss = 4.50154 (* 1 = 4.50154 loss)
I1118 21:24:18.876210 27838 sgd_solver.cpp:105] Iteration 180, lr = 0.001
I1118 21:24:29.902701 27838 solver.cpp:448] Snapshotting to binary proto file /home/hzzone/1tb/id-model/classification/3d/3d_alexnet/3d_classification_alexnet_train_iter_200.caffemodel
I1118 21:24:43.377259 27838 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/hzzone/1tb/id-model/classification/3d/3d_alexnet/3d_classification_alexnet_train_iter_200.solverstate
I1118 21:24:55.026515 27838 solver.cpp:219] Iteration 200 (0.553269 iter/s, 36.1488s/20 iters), loss = 5.02156
I1118 21:24:55.026841 27838 solver.cpp:238]     Train net output #0: loss = 5.02156 (* 1 = 5.02156 loss)
I1118 21:24:55.026854 27838 sgd_solver.cpp:105] Iteration 200, lr = 0.001
I1118 21:25:06.979553 27838 solver.cpp:219] Iteration 220 (1.67333 iter/s, 11.9522s/20 iters), loss = 4.26893
I1118 21:25:06.979629 27838 solver.cpp:238]     Train net output #0: loss = 4.26893 (* 1 = 4.26893 loss)
I1118 21:25:06.979641 27838 sgd_solver.cpp:105] Iteration 220, lr = 0.001
I1118 21:25:18.910284 27838 solver.cpp:219] Iteration 240 (1.67643 iter/s, 11.9301s/20 iters), loss = 4.5825
I1118 21:25:18.910351 27838 solver.cpp:238]     Train net output #0: loss = 4.5825 (* 1 = 4.5825 loss)
I1118 21:25:18.910362 27838 sgd_solver.cpp:105] Iteration 240, lr = 0.001
I1118 21:25:30.869356 27838 solver.cpp:219] Iteration 260 (1.67245 iter/s, 11.9585s/20 iters), loss = 4.3791
I1118 21:25:30.869534 27838 solver.cpp:238]     Train net output #0: loss = 4.3791 (* 1 = 4.3791 loss)
I1118 21:25:30.869545 27838 sgd_solver.cpp:105] Iteration 260, lr = 0.001
I1118 21:25:42.821413 27838 solver.cpp:219] Iteration 280 (1.67345 iter/s, 11.9514s/20 iters), loss = 4.31575
I1118 21:25:42.821476 27838 solver.cpp:238]     Train net output #0: loss = 4.31575 (* 1 = 4.31575 loss)
I1118 21:25:42.821487 27838 sgd_solver.cpp:105] Iteration 280, lr = 0.001
I1118 21:25:54.019273 27838 solver.cpp:448] Snapshotting to binary proto file /home/hzzone/1tb/id-model/classification/3d/3d_alexnet/3d_classification_alexnet_train_iter_300.caffemodel
I1118 21:26:07.248112 27838 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/hzzone/1tb/id-model/classification/3d/3d_alexnet/3d_classification_alexnet_train_iter_300.solverstate
I1118 21:26:19.672720 27838 solver.cpp:219] Iteration 300 (0.542745 iter/s, 36.8497s/20 iters), loss = 4.28097
I1118 21:26:19.672801 27838 solver.cpp:238]     Train net output #0: loss = 4.28097 (* 1 = 4.28097 loss)
I1118 21:26:19.672814 27838 sgd_solver.cpp:105] Iteration 300, lr = 0.001
I1118 21:26:31.935902 27838 solver.cpp:219] Iteration 320 (1.63098 iter/s, 12.2626s/20 iters), loss = 4.30978
I1118 21:26:31.935983 27838 solver.cpp:238]     Train net output #0: loss = 4.30978 (* 1 = 4.30978 loss)
I1118 21:26:31.935997 27838 sgd_solver.cpp:105] Iteration 320, lr = 0.001
I1118 21:26:44.285682 27838 solver.cpp:219] Iteration 340 (1.61954 iter/s, 12.3492s/20 iters), loss = 4.3721
I1118 21:26:44.285822 27838 solver.cpp:238]     Train net output #0: loss = 4.3721 (* 1 = 4.3721 loss)
I1118 21:26:44.285833 27838 sgd_solver.cpp:105] Iteration 340, lr = 0.001
I1118 21:26:56.576328 27838 solver.cpp:219] Iteration 360 (1.62734 iter/s, 12.29s/20 iters), loss = 4.2446
I1118 21:26:56.576408 27838 solver.cpp:238]     Train net output #0: loss = 4.2446 (* 1 = 4.2446 loss)
I1118 21:26:56.576419 27838 sgd_solver.cpp:105] Iteration 360, lr = 0.001
I1118 21:27:08.976779 27838 solver.cpp:219] Iteration 380 (1.61293 iter/s, 12.3998s/20 iters), loss = 4.13815
I1118 21:27:08.976853 27838 solver.cpp:238]     Train net output #0: loss = 4.13815 (* 1 = 4.13815 loss)
I1118 21:27:08.976866 27838 sgd_solver.cpp:105] Iteration 380, lr = 0.001
I1118 21:27:20.333376 27838 solver.cpp:448] Snapshotting to binary proto file /home/hzzone/1tb/id-model/classification/3d/3d_alexnet/3d_classification_alexnet_train_iter_400.caffemodel
I1118 21:27:33.770527 27838 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/hzzone/1tb/id-model/classification/3d/3d_alexnet/3d_classification_alexnet_train_iter_400.solverstate
I1118 21:27:46.505702 27838 solver.cpp:219] Iteration 400 (0.532945 iter/s, 37.5273s/20 iters), loss = 4.32673
I1118 21:27:46.505779 27838 solver.cpp:238]     Train net output #0: loss = 4.32673 (* 1 = 4.32673 loss)
I1118 21:27:46.505790 27838 sgd_solver.cpp:105] Iteration 400, lr = 0.001
I1118 21:27:58.576709 27838 solver.cpp:219] Iteration 420 (1.65694 iter/s, 12.0704s/20 iters), loss = 4.3798
I1118 21:27:58.577086 27838 solver.cpp:238]     Train net output #0: loss = 4.3798 (* 1 = 4.3798 loss)
I1118 21:27:58.577139 27838 sgd_solver.cpp:105] Iteration 420, lr = 0.001
I1118 21:28:10.754185 27838 solver.cpp:219] Iteration 440 (1.64249 iter/s, 12.1766s/20 iters), loss = 4.23836
I1118 21:28:10.754246 27838 solver.cpp:238]     Train net output #0: loss = 4.23836 (* 1 = 4.23836 loss)
I1118 21:28:10.754256 27838 sgd_solver.cpp:105] Iteration 440, lr = 0.001
I1118 21:28:22.900002 27838 solver.cpp:219] Iteration 460 (1.64673 iter/s, 12.1453s/20 iters), loss = 4.2623
I1118 21:28:22.900074 27838 solver.cpp:238]     Train net output #0: loss = 4.2623 (* 1 = 4.2623 loss)
I1118 21:28:22.900085 27838 sgd_solver.cpp:105] Iteration 460, lr = 0.001
I1118 21:28:35.204124 27838 solver.cpp:219] Iteration 480 (1.62555 iter/s, 12.3035s/20 iters), loss = 4.47081
I1118 21:28:35.204273 27838 solver.cpp:238]     Train net output #0: loss = 4.47081 (* 1 = 4.47081 loss)
I1118 21:28:35.204285 27838 sgd_solver.cpp:105] Iteration 480, lr = 0.001
I1118 21:28:46.435876 27838 solver.cpp:448] Snapshotting to binary proto file /home/hzzone/1tb/id-model/classification/3d/3d_alexnet/3d_classification_alexnet_train_iter_500.caffemodel
I1118 21:29:00.338539 27838 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/hzzone/1tb/id-model/classification/3d/3d_alexnet/3d_classification_alexnet_train_iter_500.solverstate
I1118 21:29:12.177069 27838 solver.cpp:219] Iteration 500 (0.540959 iter/s, 36.9714s/20 iters), loss = 4.07354
I1118 21:29:12.177191 27838 solver.cpp:238]     Train net output #0: loss = 4.07354 (* 1 = 4.07354 loss)
I1118 21:29:12.177204 27838 sgd_solver.cpp:105] Iteration 500, lr = 0.001
I1118 21:29:24.783180 27838 solver.cpp:219] Iteration 520 (1.58661 iter/s, 12.6055s/20 iters), loss = 4.23082
I1118 21:29:24.783259 27838 solver.cpp:238]     Train net output #0: loss = 4.23082 (* 1 = 4.23082 loss)
I1118 21:29:24.783272 27838 sgd_solver.cpp:105] Iteration 520, lr = 0.001
I1118 21:29:37.117023 27838 solver.cpp:219] Iteration 540 (1.62163 iter/s, 12.3333s/20 iters), loss = 4.2633
I1118 21:29:37.117100 27838 solver.cpp:238]     Train net output #0: loss = 4.2633 (* 1 = 4.2633 loss)
I1118 21:29:37.117110 27838 sgd_solver.cpp:105] Iteration 540, lr = 0.001
I1118 21:29:49.764500 27838 solver.cpp:219] Iteration 560 (1.58141 iter/s, 12.6469s/20 iters), loss = 4.38986
I1118 21:29:49.764652 27838 solver.cpp:238]     Train net output #0: loss = 4.38986 (* 1 = 4.38986 loss)
I1118 21:29:49.764663 27838 sgd_solver.cpp:105] Iteration 560, lr = 0.001
I1118 21:30:01.874620 27838 solver.cpp:219] Iteration 580 (1.6516 iter/s, 12.1095s/20 iters), loss = 4.05821
I1118 21:30:01.874699 27838 solver.cpp:238]     Train net output #0: loss = 4.05821 (* 1 = 4.05821 loss)
I1118 21:30:01.874711 27838 sgd_solver.cpp:105] Iteration 580, lr = 0.001
I1118 21:30:13.535524 27838 solver.cpp:448] Snapshotting to binary proto file /home/hzzone/1tb/id-model/classification/3d/3d_alexnet/3d_classification_alexnet_train_iter_600.caffemodel
I1118 21:30:27.576115 27838 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/hzzone/1tb/id-model/classification/3d/3d_alexnet/3d_classification_alexnet_train_iter_600.solverstate
I1118 21:30:39.645922 27838 solver.cpp:219] Iteration 600 (0.529524 iter/s, 37.7698s/20 iters), loss = 4.29351
I1118 21:30:39.645998 27838 solver.cpp:238]     Train net output #0: loss = 4.29351 (* 1 = 4.29351 loss)
I1118 21:30:39.646009 27838 sgd_solver.cpp:105] Iteration 600, lr = 0.001
